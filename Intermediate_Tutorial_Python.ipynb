{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7ZUV64BnK9naPuIr9HMDk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasseralhuryshi1111/AI-LAB---I/blob/master/Intermediate_Tutorial_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from multiprocessing import cpu_count\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from spacy import attrs\n",
        "from spacy.symbols import VERB, NOUN, ADV, ADJ\n",
        "\n",
        "TEXT_COLUMN = 'text'\n",
        "Y_COLUMN = 'author'\n",
        "\n",
        "def test_pipeline(df, nlp_pipeline, pipeline_name=''):\n",
        "    y = df[Y_COLUMN].copy()\n",
        "    X = pd.Series(df[TEXT_COLUMN])\n",
        "    # If you've done EDA, you may have noticed that the author classes aren't quite balanced.\n",
        "    # We'll use stratified splits just to be on the safe side.\n",
        "    rskf = StratifiedKFold(n_splits=5, random_state=1)\n",
        "    losses = []\n",
        "    for train_index, test_index in rskf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        nlp_pipeline.fit(X_train, y_train)\n",
        "        losses.append(metrics.log_loss(y_test, nlp_pipeline.predict_proba(X_test)))\n",
        "    print(f'{pipeline_name} kfolds log losses: {str([str(round(x, 3)) for x in sorted(losses)])}')\n",
        "    print(f'{pipeline_name} mean log loss: {round(pd.np.mean(losses), 3)}')\n",
        "\n",
        "train_df = pd.read_csv(\"../input/train.csv\", usecols=[TEXT_COLUMN, Y_COLUMN])\n",
        "\n",
        "unigram_pipe = Pipeline([\n",
        "    ('cv', CountVectorizer()),\n",
        "    ('mnb', MultinomialNB())\n",
        "                        ])\n",
        "test_pipeline(train_df, unigram_pipe, \"Unigrams only\")\n",
        "\n",
        "class UnigramPredictions(TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.unigram_mnb = Pipeline([('text', CountVectorizer()), ('mnb', MultinomialNB())])\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        # Every custom transformer requires a fit method. In this case, we want to train\n",
        "        # the naive bayes model.\n",
        "        self.unigram_mnb.fit(x, y)\n",
        "        return self\n",
        "    \n",
        "    def add_unigram_predictions(self, text_series):\n",
        "        # Resetting the index ensures the indexes equal the row numbers.\n",
        "        # This guarantees nothing will be misaligned when we merge the dataframes further down.\n",
        "        df = pd.DataFrame(text_series.reset_index(drop=True))\n",
        "        # Make unigram predicted probabilities and label them with the prediction class, aka \n",
        "        # the author.\n",
        "        unigram_predictions = pd.DataFrame(\n",
        "            self.unigram_mnb.predict_proba(text_series),\n",
        "            columns=['naive_bayes_pred_' + x for x in self.unigram_mnb.classes_]\n",
        "                                           )\n",
        "        # We only need 2 out of 3 columns, as the last is always one minus the \n",
        "        # sum of the other two. In some cases, that colinearity can actually be problematic.\n",
        "        del unigram_predictions[unigram_predictions.columns[0]]\n",
        "        df = df.merge(unigram_predictions, left_index=True, right_index=True)\n",
        "        return df\n",
        "\n",
        "    def transform(self, text_series):\n",
        "        # Every custom transformer also requires a transform method. This time we just want to \n",
        "        # provide the unigram predictions.\n",
        "        return self.add_unigram_predictions(text_series)\n",
        "\n",
        "NLP = spacy.load('en', disable=['parser', 'ner'])\n",
        "class PartOfSpeechFeatures(TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.NLP = NLP\n",
        "        # Store the number of cpus available for when we do multithreading later on\n",
        "        self.num_cores = cpu_count()\n",
        "\n",
        "    def part_of_speechiness(self, pos_counts, part_of_speech):\n",
        "        if eval(part_of_speech) in pos_counts:\n",
        "            return pos_counts[eval(part_of_speech).numerator]\n",
        "        return 0\n",
        "\n",
        "    def add_pos_features(self, df):\n",
        "        text_series = df[TEXT_COLUMN]\n",
        "        \"\"\"\n",
        "        Parse each sentence with part of speech tags. \n",
        "        Using spaCy's pipe method gives us multi-threading 'for free'. \n",
        "        This is important as this is by far the single slowest step in the pipeline.\n",
        "        If you want to test this for yourself, you can use:\n",
        "            from time import time \n",
        "            start_time = time()\n",
        "            (some code)\n",
        "            print(f'Code took {time() - start_time} seconds')\n",
        "        For faster functions the timeit module would be standard... but that's\n",
        "        meant for situations where you can wait for the function to be called 1,000 times.\n",
        "        \"\"\"\n",
        "        df['doc'] = [i for i in self.NLP.pipe(text_series.values, n_threads=self.num_cores)]\n",
        "        df['pos_counts'] = df['doc'].apply(lambda x: x.count_by(attrs.POS))\n",
        "        # We get a very minor speed boost here by using pandas built in string methods\n",
        "        # instead of df['doc'].apply(len). String processing is generally slow in python,\n",
        "        # use the pandas string methods directly where possible.\n",
        "        df['sentence_length'] = df['doc'].str.len()\n",
        "        # This next step generates the fraction of each sentence that is composed of a \n",
        "        # specific part of speech.\n",
        "        # There's admittedly some voodoo in this step. Math can be more highly optimized in python\n",
        "        # than string processing, so spaCy really stores the parts of speech as numbers. If you\n",
        "        # try >>> VERB in the console you'll get 98 as the result.\n",
        "        # The monkey business with eval() here allows us to generate several named columns\n",
        "        # without specifying in advance that {'VERB': 98}.\n",
        "        for part_of_speech in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n",
        "            df[f'{part_of_speech.lower()}iness'] = df['pos_counts'].apply(\n",
        "                lambda x: self.part_of_speechiness(x, part_of_speech))\n",
        "            df[f'{part_of_speech.lower()}iness'] /= df['sentence_length']\n",
        "        df['avg_word_length'] = (df['doc'].apply(\n",
        "            lambda x: sum([len(word) for word in x])) / df['sentence_length'])\n",
        "        return df\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        # since this transformer doesn't train a model, we don't actually need to do anything here.\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        return self.add_pos_features(df.copy())\n",
        "\n",
        "        class DropStringColumns(TransformerMixin):\n",
        "    # You may have noticed something odd about this class: there's no __init__!\n",
        "    # It's actually inherited from TransformerMixin, so it doesn't need to be declared again.\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        for col, dtype in zip(df.columns, df.dtypes):\n",
        "            if dtype == object:\n",
        "                del df[col]\n",
        "        return df\n",
        "\n",
        "        logit_all_features_pipe = Pipeline([\n",
        "        ('uni', UnigramPredictions()),\n",
        "        ('nlp', PartOfSpeechFeatures()),\n",
        "        ('clean', DropStringColumns()), \n",
        "        ('clf', LogisticRegression())\n",
        "                                     ])\n",
        "test_pipeline(train_df, logit_all_features_pipe)\n",
        "\n",
        "def generate_submission_df(trained_prediction_pipeline, test_df):\n",
        "    predictions = pd.DataFrame(\n",
        "        trained_prediction_pipeline.predict_proba(test_df.text),\n",
        "        columns=trained_prediction_pipeline.classes_\n",
        "                               )\n",
        "    predictions['id'] = test_df['id']\n",
        "    predictions.to_csv(\"submission.csv\", index=False)\n",
        "    return predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "Ob9FC3SmXH1Y",
        "outputId": "1bcb5e08-4d48-46aa-87e6-86697302f9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-2e45ffb0a8c5>\"\u001b[0;36m, line \u001b[0;32m129\u001b[0m\n\u001b[0;31m    def fit(self, x, y=None):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    }
  ]
}