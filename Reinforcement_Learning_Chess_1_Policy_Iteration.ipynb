{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/j67Vjnyd4gMxsOrltYnH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasseralhuryshi1111/AI-LAB---I/blob/master/Reinforcement_Learning_Chess_1_Policy_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3p1Lfj0AXZr",
        "outputId": "e01d4f2e-f7c1-4efc-d233-c680665ef832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-chess in /usr/local/lib/python3.7/dist-packages (0.23.11)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/arjangroen/RLC.git\n",
            "  Cloning https://github.com/arjangroen/RLC.git to /tmp/pip-req-build-chw6jlrr\n",
            "  Running command git clone -q https://github.com/arjangroen/RLC.git /tmp/pip-req-build-chw6jlrr\n",
            "Building wheels for collected packages: RLC\n",
            "  Building wheel for RLC (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for RLC: filename=RLC-0.3-py3-none-any.whl size=22568 sha256=73fa88baefdabb05bbb13cd8e7d2bad5821666e68f43c42c1188088486c3d9c3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y7xrq90j/wheels/1f/72/7a/d0e43dd015c3d2391916b6582df6b5baf241352d1435bc332b\n",
            "Successfully built RLC\n",
            "Installing collected packages: RLC\n",
            "Successfully installed RLC-0.3\n",
            "    def evaluate_state(self, state, gamma=0.9, synchronous=True):\n",
            "        \"\"\"\n",
            "        Calculates the value of a state based on the successor states and the immediate rewards.\n",
            "        Args:\n",
            "            state: tuple of 2 integers 0-7 representing the state\n",
            "            gamma: float, discount factor\n",
            "            synchronous: Boolean\n",
            "\n",
            "        Returns: The expected value of the state under the current policy.\n",
            "\n",
            "        \"\"\"\n",
            "        greedy_action_value = np.max(self.agent.policy[state[0], state[1], :])\n",
            "        greedy_indices = [i for i, a in enumerate(self.agent.policy[state[0], state[1], :]) if\n",
            "                          a == greedy_action_value]  # List of all greedy actions\n",
            "        prob = 1 / len(greedy_indices)  # probability of an action occuring\n",
            "        state_value = 0\n",
            "        for i in greedy_indices:\n",
            "            self.env.state = state  # reset state to the one being evaluated\n",
            "            reward, episode_end = self.env.step(self.agent.action_space[i])\n",
            "            if synchronous:\n",
            "                successor_state_value = self.agent.value_function_prev[self.env.state]\n",
            "            else:\n",
            "                successor_state_value = self.agent.value_function[self.env.state]\n",
            "            state_value += (prob * (\n",
            "                    reward + gamma * successor_state_value))  # sum up rewards and discounted successor state value\n",
            "        return state_value\n",
            "\n",
            "    def evaluate_policy(self, gamma=0.9, synchronous=True):\n",
            "        self.agent.value_function_prev = self.agent.value_function.copy()  # For synchronous updates\n",
            "        for row in range(self.agent.value_function.shape[0]):\n",
            "            for col in range(self.agent.value_function.shape[1]):\n",
            "                self.agent.value_function[row, col] = self.evaluate_state((row, col), gamma=gamma,\n",
            "                                                                          synchronous=synchronous)\n",
            "\n",
            "converged at iter 428\n",
            "    def improve_policy(self):\n",
            "        \"\"\"\n",
            "        Finds the greedy policy w.r.t. the current value function\n",
            "        \"\"\"\n",
            "\n",
            "        self.agent.policy_prev = self.agent.policy.copy()\n",
            "        for row in range(self.agent.action_function.shape[0]):\n",
            "            for col in range(self.agent.action_function.shape[1]):\n",
            "                for action in range(self.agent.action_function.shape[2]):\n",
            "                    self.env.state = (row, col)  # reset state to the one being evaluated\n",
            "                    reward, episode_end = self.env.step(self.agent.action_space[action])\n",
            "                    successor_state_value = 0 if episode_end else self.agent.value_function[self.env.state]\n",
            "                    self.agent.policy[row, col, action] = reward + successor_state_value\n",
            "\n",
            "                max_policy_value = np.max(self.agent.policy[row, col, :])\n",
            "                max_indices = [i for i, a in enumerate(self.agent.policy[row, col, :]) if a == max_policy_value]\n",
            "                for idx in max_indices:\n",
            "                    self.agent.policy[row, col, idx] = 1\n",
            "\n",
            "[['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↓', '↙', '↙'],\n",
            " ['→', '→', '→', '→', '→', 'F', '←', '←']]\n",
            "    def policy_iteration(self, eps=0.1, gamma=0.9, iteration=1, k=32, synchronous=True):\n",
            "        \"\"\"\n",
            "        Finds the optimal policy\n",
            "        Args:\n",
            "            eps: float, exploration rate\n",
            "            gamma: float, discount factor\n",
            "            iteration: the iteration number\n",
            "            k: (int) maximum amount of policy evaluation iterations\n",
            "            synchronous: (Boolean) whether to use synchronous are asynchronous back-ups \n",
            "\n",
            "        Returns:\n",
            "\n",
            "        \"\"\"\n",
            "        policy_stable = True\n",
            "        print(\"\\n\\n______iteration:\", iteration, \"______\")\n",
            "        print(\"\\n policy:\")\n",
            "        self.visualize_policy()\n",
            "\n",
            "        print(\"\")\n",
            "        value_delta_max = 0\n",
            "        for _ in range(k):\n",
            "            self.evaluate_policy(gamma=gamma, synchronous=synchronous)\n",
            "            value_delta = np.max(np.abs(self.agent.value_function_prev - self.agent.value_function))\n",
            "            value_delta_max = value_delta\n",
            "            if value_delta_max < eps:\n",
            "                break\n",
            "        print(\"Value function for this policy:\")\n",
            "        print(self.agent.value_function.round().astype(int))\n",
            "        action_function_prev = self.agent.action_function.copy()\n",
            "        print(\"\\n Improving policy:\")\n",
            "        self.improve_policy()\n",
            "        policy_stable = self.agent.compare_policies() < 1\n",
            "        print(\"policy diff:\", policy_stable)\n",
            "\n",
            "        if not policy_stable and iteration < 1000:\n",
            "            iteration += 1\n",
            "            self.policy_iteration(iteration=iteration)\n",
            "        elif policy_stable:\n",
            "            print(\"Optimal policy found in\", iteration, \"steps of policy evaluation\")\n",
            "        else:\n",
            "            print(\"failed to converge.\")\n",
            "\n",
            "\n",
            "\n",
            "______iteration: 1 ______\n",
            "\n",
            " policy:\n",
            "[['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↓', '↙', '↙'],\n",
            " ['→', '→', '→', '→', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-5 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-5 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-4 -4 -4 -4 -4 -4 -4 -4]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -2 -2 -2 -2 -2]\n",
            " [-4 -3 -3 -2 -1 -1 -1 -2]\n",
            " [-4 -3 -3 -2 -1  0 -1 -2]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: False\n",
            "\n",
            "\n",
            "______iteration: 2 ______\n",
            "\n",
            " policy:\n",
            "[['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['→', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↗', '→', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↗', '↗', '→', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↗', '↗', '↗', '→', '↘', '↓', '↙', '↙'],\n",
            " ['↗', '↗', '↗', '↗', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-5 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-5 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-4 -4 -4 -4 -4 -4 -4 -4]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -2 -2 -2 -2 -2]\n",
            " [-4 -3 -3 -2 -1 -1 -1 -2]\n",
            " [-4 -3 -3 -2 -1  0 -1 -2]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: True\n",
            "Optimal policy found in 2 steps of policy evaluation\n",
            "\n",
            "\n",
            "______iteration: 1 ______\n",
            "\n",
            " policy:\n",
            "[['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', 'F', '↑', '↑']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-47 -48 -48 -49 -48 -48 -48 -48]\n",
            " [-48 -49 -49 -49 -49 -49 -49 -49]\n",
            " [-49 -49 -49 -49 -49 -49 -48 -48]\n",
            " [-49 -49 -49 -49 -48 -47 -47 -47]\n",
            " [-49 -49 -48 -47 -46 -45 -44 -44]\n",
            " [-48 -48 -47 -44 -42 -40 -39 -39]\n",
            " [-48 -48 -46 -42 -34 -31 -31 -35]\n",
            " [-48 -48 -45 -41 -31   0 -27 -33]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: False\n",
            "\n",
            "\n",
            "______iteration: 2 ______\n",
            "\n",
            " policy:\n",
            "[['↑', '←', '←', '←', '→', '→', '→', '↑'],\n",
            " ['↑', '↖', '↖', '↖', '↗', '↗', '↗', '↑'],\n",
            " ['↑', '↖', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↓', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↓', '↙', '↙'],\n",
            " ['→', '→', '→', '→', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-11 -11 -11 -11 -11 -11 -11 -11]\n",
            " [-11 -11 -11 -11 -11 -11 -11 -11]\n",
            " [-11 -11  -4  -4  -4  -4  -4  -4]\n",
            " [ -5  -3  -3  -3  -3  -3  -3  -3]\n",
            " [ -4  -3  -3  -3  -3  -3  -3  -3]\n",
            " [ -4  -3  -3  -2  -2  -2  -2  -2]\n",
            " [ -4  -3  -3  -2  -1  -1  -1  -2]\n",
            " [ -4  -3  -3  -2  -1   0  -1  -2]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: False\n",
            "\n",
            "\n",
            "______iteration: 3 ______\n",
            "\n",
            " policy:\n",
            "[['↑', '↑', '↑', '↑', '↙', '↑', '↑', '↑'],\n",
            " ['↑', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['→', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↗', '→', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↗', '↗', '→', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↗', '↗', '↗', '→', '↘', '↓', '↙', '↙'],\n",
            " ['↗', '↗', '↗', '↗', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-7 -6 -6 -6 -6 -6 -6 -6]\n",
            " [-6 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-4 -4 -4 -4 -4 -4 -4 -4]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -2 -2 -2 -2 -2]\n",
            " [-4 -3 -3 -2 -1 -1 -1 -2]\n",
            " [-4 -3 -3 -2 -1  0 -1 -2]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: False\n",
            "\n",
            "\n",
            "______iteration: 4 ______\n",
            "\n",
            " policy:\n",
            "[['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['→', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↗', '→', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↗', '↗', '→', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↗', '↗', '↗', '→', '↘', '↓', '↙', '↙'],\n",
            " ['↗', '↗', '↗', '↗', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-5 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-5 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-4 -4 -4 -4 -4 -4 -4 -4]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -2 -2 -2 -2 -2]\n",
            " [-4 -3 -3 -2 -1 -1 -1 -2]\n",
            " [-4 -3 -3 -2 -1  0 -1 -2]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: False\n",
            "\n",
            "\n",
            "______iteration: 5 ______\n",
            "\n",
            " policy:\n",
            "[['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['→', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↗', '→', '↘', '↘', '↘', '↘', '↘', '↓'],\n",
            " ['↗', '↗', '→', '↘', '↘', '↘', '↓', '↙'],\n",
            " ['↗', '↗', '↗', '→', '↘', '↓', '↙', '↙'],\n",
            " ['↗', '↗', '↗', '↗', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-5 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-5 -5 -5 -5 -5 -5 -5 -5]\n",
            " [-4 -4 -4 -4 -4 -4 -4 -4]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -3 -3 -3 -3 -3]\n",
            " [-4 -3 -3 -2 -2 -2 -2 -2]\n",
            " [-4 -3 -3 -2 -1 -1 -1 -2]\n",
            " [-4 -3 -3 -2 -1  0 -1 -2]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: True\n",
            "Optimal policy found in 5 steps of policy evaluation\n",
            "\n",
            "\n",
            "______iteration: 1 ______\n",
            "\n",
            " policy:\n",
            "[['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', 'F', '↑', '↑']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1 -1 -1 -1 -1  0 -1 -1]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: False\n",
            "\n",
            "\n",
            "______iteration: 2 ______\n",
            "\n",
            " policy:\n",
            "[['↑', '↑', '↑', '↑', '↑', '↓', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↓', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↓', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↓', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↓', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↓', '↑', '↑'],\n",
            " ['↑', '↑', '↑', '↑', '↑', '↓', '↑', '↑'],\n",
            " ['→', '→', '→', '→', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-6 -6 -6 -6 -6 -1 -6 -6]\n",
            " [-6 -6 -6 -6 -6 -1 -6 -6]\n",
            " [-6 -6 -6 -6 -6 -1 -6 -6]\n",
            " [-6 -6 -6 -6 -6 -1 -6 -6]\n",
            " [-6 -6 -6 -6 -6 -1 -6 -6]\n",
            " [-6 -6 -6 -6 -6 -1 -6 -6]\n",
            " [-6 -6 -6 -6 -6 -1 -6 -6]\n",
            " [-1 -1 -1 -1 -1  0 -1 -1]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: False\n",
            "\n",
            "\n",
            "______iteration: 3 ______\n",
            "\n",
            " policy:\n",
            "[['→', '→', '→', '→', '→', '↓', '←', '←'],\n",
            " ['→', '→', '→', '→', '→', '↓', '←', '←'],\n",
            " ['→', '→', '→', '→', '→', '↓', '←', '←'],\n",
            " ['↓', '→', '→', '→', '→', '↓', '←', '←'],\n",
            " ['↓', '↓', '→', '→', '→', '↓', '←', '←'],\n",
            " ['↓', '↓', '↓', '→', '→', '↓', '←', '↓'],\n",
            " ['↓', '↓', '↓', '↓', '→', '↓', '↓', '↓'],\n",
            " ['→', '→', '→', '→', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-1 -1 -1 -1 -1  0 -1 -1]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: False\n",
            "\n",
            "\n",
            "______iteration: 4 ______\n",
            "\n",
            " policy:\n",
            "[['→', '→', '→', '→', '→', '↓', '←', '←'],\n",
            " ['→', '→', '→', '→', '→', '↓', '←', '←'],\n",
            " ['→', '→', '→', '→', '→', '↓', '←', '←'],\n",
            " ['↓', '→', '→', '→', '→', '↓', '←', '←'],\n",
            " ['↓', '↓', '→', '→', '→', '↓', '←', '←'],\n",
            " ['↓', '↓', '↓', '→', '→', '↓', '←', '↓'],\n",
            " ['↓', '↓', '↓', '↓', '→', '↓', '↓', '↓'],\n",
            " ['→', '→', '→', '→', '→', 'F', '←', '←']]\n",
            "\n",
            "Value function for this policy:\n",
            "[[-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-2 -2 -2 -2 -2 -1 -2 -2]\n",
            " [-1 -1 -1 -1 -1  0 -1 -1]]\n",
            "\n",
            " Improving policy:\n",
            "policy diff: True\n",
            "Optimal policy found in 4 steps of policy evaluation\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import inspect\n",
        "\n",
        "!pip install python-chess  # Python-Chess is the Python Chess Package that handles the chess environment\n",
        "!pip install --upgrade git+https://github.com/arjangroen/RLC.git  # RLC is the Reinforcement Learning package\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from RLC.move_chess.environment import Board\n",
        "from RLC.move_chess.agent import Piece\n",
        "from RLC.move_chess.learn import Reinforce\n",
        "\n",
        "env = Board()\n",
        "env.render()\n",
        "env.visual_board\n",
        "\n",
        "p = Piece(piece='king')\n",
        "\n",
        "r = Reinforce(p,env)\n",
        "\n",
        "print(inspect.getsource(r.evaluate_state))\n",
        "\n",
        "r.agent.value_function.astype(int)\n",
        "\n",
        "state = (0,0)\n",
        "r.agent.value_function[0,0] = r.evaluate_state(state,gamma=1)\n",
        "\n",
        "r.agent.value_function.astype(int)\n",
        "\n",
        "print(inspect.getsource(r.evaluate_policy))\n",
        "\n",
        "r.evaluate_policy(gamma=1)\n",
        "\n",
        "r.agent.value_function.astype(int)\n",
        "\n",
        "eps=0.1\n",
        "k_max = 1000\n",
        "value_delta_max = 0\n",
        "gamma = 1\n",
        "synchronous=True\n",
        "value_delta_max = 0\n",
        "for k in range(k_max):\n",
        "    r.evaluate_policy(gamma=gamma,synchronous=synchronous)\n",
        "    value_delta = np.max(np.abs(r.agent.value_function_prev - r.agent.value_function))\n",
        "    value_delta_max = value_delta\n",
        "    if value_delta_max < eps:\n",
        "        print('converged at iter',k)\n",
        "        break\n",
        "\n",
        "r.agent.value_function.astype(int)\n",
        "\n",
        "print(inspect.getsource(r.improve_policy))\n",
        "\n",
        "r.improve_policy()\n",
        "r.visualize_policy()\n",
        "\n",
        "print(inspect.getsource(r.policy_iteration))\n",
        "\n",
        "r.policy_iteration()\n",
        "\n",
        "agent = Piece(piece='king')\n",
        "r = Reinforce(agent,env)\n",
        "\n",
        "r.policy_iteration(gamma=1,synchronous=False)\n",
        "\n",
        "r.agent.value_function.astype(int)\n",
        "\n",
        "agent = Piece(piece='rook')  # Let's pick a rook for a change.\n",
        "r = Reinforce(agent,env)\n",
        "r.policy_iteration(k=1,gamma=1)  # The only difference here is that we set k_max to 1."
      ]
    }
  ]
}